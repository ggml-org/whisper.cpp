# -*- coding: utf-8 -*-
"""Whisper tiny.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RBTHR-XbcUxBP3XMVSMnjq-kKxIpypdv
"""

!pip -q install faster-whisper==1.0.3 rapidfuzz==3.9.6 python-Levenshtein==0.25.1 soundfile==0.12.1
!ffmpeg -version >/dev/null 2>&1 || (sudo apt-get update -y && sudo apt-get install -y ffmpeg)

# Click Start, say: â€œcall Jagatheeswaranâ€, then Stop. It'll save /content/rec.wav.

import base64, os
from google.colab import output
from IPython.display import HTML, Audio, display

def _save_audio(b64_webm):
    raw = base64.b64decode(b64_webm)
    with open('/content/rec.webm', 'wb') as f:
        f.write(raw)
    os.system("ffmpeg -y -i /content/rec.webm -ac 1 -ar 16000 /content/rec.wav -loglevel error")

output.register_callback('notebook.saveAudio', _save_audio)

js = r"""
(() => {
  // remove any previous UI
  const old = document.getElementById('colab-rec-ui');
  if (old) old.remove();

  // UI
  const wrap = document.createElement('div');
  wrap.id = 'colab-rec-ui';
  wrap.style.cssText = 'margin:12px 0;padding:12px;border:1px solid #444;border-radius:10px';

  const start = document.createElement('button');
  start.textContent = 'ðŸŽ™ï¸ Start';
  start.style.cssText = 'padding:8px 16px;margin-right:8px';

  const stop = document.createElement('button');
  stop.textContent = 'â¹ï¸ Stop';
  stop.style.cssText = 'padding:8px 16px';
  stop.disabled = true;

  const status = document.createElement('div');
  status.style.cssText = 'margin-top:8px;color:#0b5';
  status.textContent = 'Ready';

  wrap.appendChild(start);
  wrap.appendChild(stop);
  wrap.appendChild(status);
  document.body.appendChild(wrap);

  let rec, chunks = [], stream;

  start.onclick = async () => {
    chunks = [];
    stream = await navigator.mediaDevices.getUserMedia({audio:true});
    rec = new MediaRecorder(stream);
    rec.ondataavailable = e => chunks.push(e.data);
    rec.onstop = async () => {
      const blob = new Blob(chunks, {type:'audio/webm'});
      const buf = await blob.arrayBuffer();
      const b64 = btoa(String.fromCharCode(...new Uint8Array(buf)));
      google.colab.kernel.invokeFunction('notebook.saveAudio', [b64], {});
      stream.getTracks().forEach(t => t.stop());
      status.textContent = 'Saved â†’ /content/rec.wav';
    };
    rec.start();
    status.textContent = 'Recordingâ€¦';
    start.disabled = true;
    stop.disabled = false;
  };

  stop.onclick = () => {
    if (rec && rec.state === 'recording') rec.stop();
    start.disabled = false;
    stop.disabled = true;
  };

  return null; // IMPORTANT: return a simple value, not a DOM node
})();
"""

display(HTML("<div/>"))
output.eval_js(js)

# Optional: quick player after you record
if os.path.exists("/content/rec.wav"):
    display(Audio("/content/rec.wav"))
else:
    print("Record first, then re-run this cell to preview.")

from IPython.display import Audio, display
if os.path.exists("/content/rec.wav"):
    display(Audio("/content/rec.wav"))
else:
    print("No recording yet. Run the recorder cell first.")

from faster_whisper import WhisperModel
import time, os

AUDIO_PATH = "/content/rec.wav"  # change to a local WAV path if you prefer

assert os.path.exists(AUDIO_PATH), "No audio found. Record first or set AUDIO_PATH to an existing .wav"

model = WhisperModel("tiny.en", compute_type="float32")  # fallback to "float32" if needed

t0 = time.time()
segments, info = model.transcribe(AUDIO_PATH, beam_size=1, vad_filter=True)
hyp = "".join(seg.text for seg in segments).strip()
lat_ms = round((time.time() - t0)*1000, 1)

print("---- TRANSCRIPT (raw) ----")
print(hyp)
print(f"\nLatency: {lat_ms} ms")

from faster_whisper import WhisperModel
import time, os, re, unicodedata
from rapidfuzz import fuzz

AUDIO_PATH = "/content/rec.wav"  # your recorded/test wav

assert os.path.exists(AUDIO_PATH), "No audio found. Record first or set AUDIO_PATH to an existing .wav"

# ---- init model (same as before) ----
model = WhisperModel("base.en", compute_type="float32")  # you can change to "tiny" if multilingual

# ---- helpers ----
def norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", s).lower().strip()
    s = re.sub(r"\s+", " ", s)
    return s

# --- make extraction robust to "call, venkatesh" and similar ---
CALL_PATTERNS = [
    r'^(?:please )?(?:call|dial|phone)[\s,;:]+(?P<name>.+)$',            # call, venkatesh
    r'^(?:please )?(?:call|dial|phone)\s+to\s+(?P<name>.+)$',            # call to venkatesh
    r'^(?P<name>.+?)(?: ko)? (?:call|à¤•à¥‰à¤²)(?: (?:à¤•à¤°à¥‹|à¤²à¤—à¤¾à¤“))?$',            # venkatesh ko call karo
    r'^call\s+(?P<name>.+?)(?: (?:à¤•à¤°à¥‹|à¤²à¤—à¤¾à¤“))?$'                          # call venkatesh
]

def extract_name(text: str):
    t = norm(text)
    # calm down punctuation inside the line (keep spaces)
    t = re.sub(r"[,\.;:]+", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    for p in CALL_PATTERNS:
        m = re.match(p, t)
        if m:
            return re.sub(r'^(please )+', '', m.group("name").strip())
    # fallback: tokens after 'call'
    toks = t.split()
    for i,w in enumerate(toks):
        if w in {"call","dial","phone","à¤•à¥‰à¤²"} and i+1 < len(toks):
            return " ".join(toks[i+1:i+4])
    return None

def fuzzy_best(q: str, candidates: list):
    if not q or not candidates: return None, 0
    qn = norm(q)
    best, score = None, -1
    for cand in candidates:
        sc = fuzz.token_set_ratio(qn, norm(cand))
        if sc > score:
            best, score = cand, sc
    return best, score

def apply_correction(text, q, chosen):
    if not (text and q and chosen): return text
    return norm(text).replace(norm(q), norm(chosen), 1)

# ---- contacts lexicon ----
CONTACTS = ["Jitesh","Pratyush","Jagatheeswaran","Akshay Kumar","Priya","Venkatesh","Nandini"]

# ---- first pass transcription ----
t0 = time.time()
segments, info = model.transcribe(AUDIO_PATH, beam_size=1, vad_filter=True)
hyp = "".join(seg.text for seg in segments).strip()
lat_ms = round((time.time() - t0)*1000, 1)

print("---- TRANSCRIPT (raw) ----")
print(hyp)
print(f"Latency: {lat_ms} ms")

# ---- try to match directly ----
qname = extract_name(hyp)
best, score = fuzzy_best(qname, CONTACTS) if qname else (None, 0)
print("\nExtracted name:", qname)
print("Best direct match:", best, "score=", score)

if best and score >= 85:
    corrected = apply_correction(hyp, qname, best)
    print("\nCORRECTED (direct):", corrected)
else:
    # ---- prompt nudged re-decode ----
    topN = ", ".join(CONTACTS[:8])  # pick top-8 or fuzzy top-N
    prompt = f"Transcribe a short command like 'call <name>'. Likely names are: {topN}."
    t1 = time.time()
    segs2, _ = model.transcribe(AUDIO_PATH, beam_size=3, vad_filter=True, initial_prompt=prompt)
    hyp2 = "".join(s.text for s in segs2).strip()
    lat2 = round((time.time()-t1)*1000,1)

    print("\n---- TRANSCRIPT (nudged) ----")
    print(hyp2)
    print(f"Extra Latency: {lat2} ms")

    qname2 = extract_name(hyp2) or qname
    best2, score2 = fuzzy_best(qname2, CONTACTS) if qname2 else (None, 0)
    print("Best nudged match:", best2, "score=", score2)

    if best2 and score2 >= 80:
        corrected = apply_correction(hyp2, qname2, best2)
        print("\nCORRECTED (nudged):", corrected)
    else:
        corrected = hyp  # fallback
        print("\n(No reliable correction.)")

# ---- final result ----
print("\n==== FINAL RESULT ====")
print(corrected)

import re, unicodedata
from rapidfuzz import fuzz

# Minimal normalization
def norm(s):
    s = unicodedata.normalize("NFKC", s).lower().strip()
    s = re.sub(r"\s+", " ", s)
    return s

# Simple patterns (English/Hinglish/Hindi)
CALL_PATTERNS = [
    r'^(?:please )?(?:call|dial|phone)\s+(?P<name>.+)$',
    r'^(?P<name>.+?)(?: ko)? (?:call|à¤•à¥‰à¤²)(?: (?:à¤•à¤°à¥‹|à¤²à¤—à¤¾à¤“))?$',
    r'^call\s+(?P<name>.+?)(?: (?:à¤•à¤°à¥‹|à¤²à¤—à¤¾à¤“))?$'
]

def extract_name(text):
    t = norm(text)
    t = re.sub(r"( on (mobile|home|office))$", "", t)
    t = re.sub(r"[\,\.\!]+$", "", t)
    for p in CALL_PATTERNS:
        m = re.match(p, t)
        if m:
            return m.group("name").strip()
    return None

# Tiny starter lexicon (replace with your 1000-name list)
LEXICON = [
  "Jagatheeswaran Senthilvelan", "Prathyush", "Akshay","Prakhar Gupta", "Hamish", "Aarav", "Karthik", "Jitesh","Venkatesh",
  "Priya", "Nandini", "Rahul", "Rakesh", "Rohit", "Sanjay", "Varun", "Vijay", "Jagdish", "Ayush"
]

hyp_name = extract_name(hyp) if hyp else None

print("\n---- NAME PARSE ----")
print("Hypothesized name:", hyp_name)

def best_match(q, candidates):
    if not q: return None, 0
    qn = norm(q)
    best, score = None, -1
    for c in candidates:
        sc = fuzz.token_set_ratio(qn, norm(c))
        if sc > score:
            best, score = c, sc
    return best, score

best, score = best_match(hyp_name, LEXICON) if hyp_name else (None, 0)

print("\n---- MATCHED CONTACT ----")
print("Best:", best)
print("Score:", score, "(>=85 is usually safe)")

# Build a corrected command: replace the raw name with the matched one (lowercased to keep simple)
corrected = hyp
if hyp_name and best and score >= 85:
    corrected = norm(hyp).replace(norm(hyp_name), norm(best), 1)

print("\n---- CORRECTED COMMAND ----")
print(corrected or "(no change)")

# --- improved name extraction + matching (drop-in) ---
!pip -q install rapidfuzz==3.9.6 python-Levenshtein==0.25.1 metaphone==0.6

import re, unicodedata
from rapidfuzz import fuzz
from metaphone import doublemetaphone

def norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", s).strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s

# Broader patterns (English / Hinglish / Hindi)
CALL_PATTERNS = [
    r'^(?:please )?(?:call|dial|phone|thanks|text)\s+(?P<name>.+)$',                     # call pratyush
    r'^(?:please )?(?:call|dial|phone)\s+to\s+(?P<name>.+)$',               # call to pratyush
    r'^(?P<name>.+?)(?: ko)? (?:call|à¤•à¥‰à¤²)(?: (?:à¤•à¤°à¥‹|à¤²à¤—à¤¾à¤“))?$',              # pratyush ko call karo
    r'^call\s+(?P<name>.+?)(?: (?:à¤•à¤°à¥‹|à¤²à¤—à¤¾à¤“))?$',                            # call pratyush karo
]

def extract_name(text: str):
    t = norm(text)
    # drop common tails (e.g., "on mobile/home/office")
    t = re.sub(r"( on (mobile|home|office))$", "", t)
    t = re.sub(r"[,\.\!]+$", "", t)
    for p in CALL_PATTERNS:
        m = re.match(p, t)
        if m:
            name = m.group("name").strip()
            # remove polite fillers
            name = re.sub(r'^(please )+', '', name).strip()
            return name if name else None
    return None

def dm_keys(s: str):
    a,b = doublemetaphone(s)
    return {a,b} - {None,""}

def match_score(q: str, cand: str) -> float:
    # Hybrid: phonetic (en), fuzzy, token overlap
    qn, cn = norm(q), norm(cand)
    phon = 1.0 if dm_keys(qn) & dm_keys(cn) else 0.0
    fuz  = fuzz.token_set_ratio(qn, cn) / 100.0
    qt, ct = set(qn.split()), set(cn.split())
    tok  = len(qt & ct) / max(1, len(qt | ct))
    return 0.55*phon + 0.35*fuz + 0.10*tok  # small bump to fuzzy

def best_matches(q: str, lexicon: list, topk=5):
    if not q or not lexicon: return []
    # quick pre-filter by first letter or length proximity
    qn = norm(q); q0 = qn[:1]
    pool = [n for n in lexicon if norm(n)[:1]==q0 or abs(len(norm(n))-len(qn))<=4]
    if len(pool) < 50:
        pool = lexicon
    scored = [(cand, match_score(q, cand)) for cand in pool]
    scored.sort(key=lambda x: x[1], reverse=True)
    return scored[:topk]

# ---- Load your lexicon ----
# If you already have a big file, use this:
# LEXICON = [ln.strip() for ln in open("/content/names_lexicon.txt", encoding="utf-8").read().splitlines() if ln.strip()]

# For quick testing, ensure 'Pratyush' is present here:
LEXICON = [
    "Pratyush", "Pratyush Kumar", "Pratyusha",  # add your expected variants
    "Jagatheeswaran Senthilvelan","Akshay","Hamish","Aarav","Aditi","Karthik", "Abhineet","Jitesh",
    "Venkatesh","Priya","Nandini","Rahul","Rakesh","Rohit","Sanjay","Varun","Vijay","Ankita","Ayush"
]

# ---- DEBUG: show basics ----
print("RAW transcript:", hyp)
extracted = extract_name(hyp)
print("Extracted name:", extracted)


# See top-5 candidates with scores
tops = best_matches(extracted, LEXICON, topk=5) if extracted else []
print("\nTop candidates:")
for cand, sc in tops:
    print(f"  {cand:30s}  score={sc:.3f}")

# Choose best and build corrected command
THRESHOLD = 0.72  # a bit gentler than 0.85% fuzzy; works well for Indian names
best, best_sc = (tops[0] if tops else (None, 0.0))
if extracted and best and best_sc >= THRESHOLD:
    corrected = norm(hyp).replace(norm(extracted), norm(best), 1)
else:
    corrected = hyp  # no change

print("\nBEST:", best, "score=", round(best_sc,3))
print("CORRECTED:", corrected)

# Robust name extraction + lexicon fallback for "call <name>" commands
# Works for English / Hinglish / Hindi phrasings and messy ASR like "call to page".
# Requires: rapidfuzz, metaphone (already installed in earlier cells)

import re, unicodedata
from rapidfuzz import fuzz
from metaphone import doublemetaphone

# ---- tweakable bits ----
THRESHOLD = 0.70          # accept match >= 0.70 (hybrid score ~ 0..1)
WINDOW_TOKENS = 3         # look up to 3 tokens after 'call' for a candidate
LEXICON = [
    # make sure your target names are here
    "Jitesh", "Jitesh Kumar",
    "Jagatheeswaran Senthilvelan","Akshay","Hamish","Aarav","Aditi","Karthik",
    "Venkatesh","Priya","Nandini","Rahul","Rakesh","Rohit","Sanjay","Varun","Vijay","Ankita","Pratyush"
]
# Or load a big file:
# LEXICON = [ln.strip() for ln in open("/content/names_lexicon.txt", encoding="utf-8") if ln.strip()]

def norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", s).lower().strip()
    s = re.sub(r"\s+", " ", s)
    return s

def dm_keys(s: str):
    a,b = doublemetaphone(s)
    return {a,b} - {None,""}

def hybrid_score(q: str, cand: str) -> float:
    # Hybrid: phonetic (English-ish) + fuzzy + token overlap
    qn, cn = norm(q), norm(cand)
    phon = 1.0 if dm_keys(qn) & dm_keys(cn) else 0.0
    fuz  = fuzz.token_set_ratio(qn, cn) / 100.0
    qt, ct = set(qn.split()), set(cn.split())
    tok  = len(qt & ct) / max(1, len(qt | ct))
    return 0.55*phon + 0.35*fuz + 0.10*tok

# 1) Primary regex extractor (more generous)
CALL_PATTERNS = [
    r'^(?:please )?(?:call|dial|phone)\s+(?P<name>.+)$',                      # call jitesh
    r'^(?:please )?(?:call|dial|phone)\s+to\s+(?P<name>.+)$',                # call to jitesh
    r'^(?P<name>.+?)(?: ko)? (?:call|à¤•à¥‰à¤²)(?: (?:à¤•à¤°à¥‹|à¤²à¤—à¤¾à¤“))?$',               # jitesh ko call karo
    r'^call\s+(?P<name>.+?)(?: (?:à¤•à¤°à¥‹|à¤²à¤—à¤¾à¤“))?$',                             # call jitesh karo
]
def extract_by_regex(text: str):
    t = norm(text)
    t = re.sub(r"( on (mobile|home|office))$", "", t)
    t = re.sub(r"[,\.\!]+$", "", t)
    for p in CALL_PATTERNS:
        m = re.match(p, t)
        if m:
            name = re.sub(r'^(please )+', '', m.group("name").strip())
            return name if name else None
    return None

# 2) Lexicon-guided fallback: find best name near the word "call"
def extract_by_window(text: str):
    t = norm(text)
    tokens = t.split()
    # words that indicate call intent
    call_words = {"call","dial","phone","à¤•à¥‰à¤²"}
    # find first call-like token
    try:
        idx = next(i for i,w in enumerate(tokens) if w in call_words)
    except StopIteration:
        idx = None
    # candidate span: tokens after 'call'
    candidate = None
    if idx is not None and idx+1 < len(tokens):
        span = " ".join(tokens[idx+1: idx+1+WINDOW_TOKENS]).strip()
        candidate = span if span else None
    return candidate

def best_match(q: str, lexicon: list):
    if not q or not lexicon: return None, 0.0
    # small pre-filter to speed up
    qn = norm(q)
    pool = [n for n in lexicon if abs(len(norm(n)) - len(qn)) <= 6] or lexicon
    best, sc = None, -1.0
    for cand in pool:
        s = hybrid_score(qn, cand)
        if s > sc:
            best, sc = cand, s
    return best, sc

# ---------- RUN ----------
print("RAW transcript:", hyp)

# A) try regex
name_rx = extract_by_regex(hyp)

# B) if regex fails, try tokens after 'call'
name_win = extract_by_window(hyp) if not name_rx else None

# C) choose the â€œquery nameâ€ to match
query_name = name_rx or name_win

# D) if still nothing, brute-force: try matching any lexicon name against the whole hyp
if not query_name:
    # pick the lexicon name that best matches anywhere in the hypothesis
    # (good when ASR turned the name into a different word entirely)
    cand, sc = best_match(hyp, LEXICON)
    query_name = cand if sc >= THRESHOLD else None
    print(f"[Brute] best-in-hyp: {cand} (score={sc:.3f})")

print("Extracted (query) name:", query_name)

best, score = best_match(query_name, LEXICON) if query_name else (None, 0.0)

print("\nTop match:", best, f"(score={score:.3f})")
if best and score >= THRESHOLD:
    corrected = norm(hyp)
    # replace only once (handles 'call to X' and similar)
    corrected = re.sub(re.escape(norm(query_name)), norm(best), corrected, count=1)
else:
    corrected = hyp

print("\nCORRECTED:", corrected)

# Load one-name-per-line file from Drive or /content
LEXICON = [ln.strip() for ln in open("/content/names_lexicon.txt", encoding="utf-8").read().splitlines() if ln.strip()]
print("Loaded", len(LEXICON), "names.")

